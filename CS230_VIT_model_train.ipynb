{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount drive to access files and directories\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCTAeohVYNLy",
        "outputId": "41f30ddc-b1c3-47ba-fdd0-08c99af08911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary libraries to patchify images\n",
        "!pip install patchify\n",
        "!pip install import-ipynb"
      ],
      "metadata": {
        "id": "tfXJI6ifbpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V-_LPTDV5HV"
      },
      "outputs": [],
      "source": [
        "# import all necessary libraries\n",
        "import os\n",
        "import import_ipynb\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from patchify import patchify\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load ViT function from vit_cs230 python file\n",
        "from vit_cs230 import ViT"
      ],
      "metadata": {
        "id": "BA1PGRg-eK1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyperparameters\n",
        "hp = {}\n",
        "hp[\"image_size\"] = 200\n",
        "hp[\"num_channels\"] = 3\n",
        "hp[\"patch_size\"] = 25\n",
        "hp[\"num_patches\"] = (hp[\"image_size\"]**2) // (hp[\"patch_size\"]**2)\n",
        "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"])\n",
        "\n",
        "hp[\"batch_size\"] = 32\n",
        "hp[\"lr\"] = 1e-4\n",
        "hp[\"num_epochs\"] = 500\n",
        "hp[\"num_classes\"] = 20\n",
        "hp[\"class_names\"] = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
        "                     '11', '12', '13', '14', '15', '16', '17', '18', '19']\n",
        "\n",
        "hp[\"num_layers\"] = 12\n",
        "hp[\"hidden_dim\"] = 768\n",
        "hp[\"mlp_dim\"] = 3072\n",
        "hp[\"num_heads\"] = 12\n",
        "hp[\"dropout_rate\"] = 0.1"
      ],
      "metadata": {
        "id": "HRSCjzYHZ5eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to patchify images and load them and labels into dataset\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# we chose a 60/20/20 split for this VIT model\n",
        "def load_data(path, split=0.2):\n",
        "    images = shuffle(glob(os.path.join(path, \"*\", \"*\")))\n",
        "    split_size = int(len(images) * split)\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "    return train_x, valid_x, test_x\n",
        "\n",
        "def process_image_label(path):\n",
        "    # read images\n",
        "    path = path.decode()\n",
        "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
        "    image = image/255.0\n",
        "\n",
        "    # patchify\n",
        "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
        "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
        "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
        "    patches = patches.astype(np.float32)\n",
        "\n",
        "    # get label from name of folder containing images\n",
        "    class_name = path.split(\"/\")[-2]\n",
        "    class_idx = hp[\"class_names\"].index(class_name)\n",
        "    class_idx = np.array(class_idx, dtype=np.int32)\n",
        "\n",
        "    return patches, class_idx\n",
        "\n",
        "def parse(path):\n",
        "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
        "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
        "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
        "    labels.set_shape(hp[\"num_classes\"])\n",
        "    return patches, labels\n",
        "\n",
        "def tf_dataset(images, batch=32): # ensure compatibility with earlier declaration\n",
        "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
        "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "B6Mg5rn_Z3VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # set the seeding\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # create directory for saving model files\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    # declare the paths\n",
        "    dataset_path = \"/content/mured_dataset/\"\n",
        "    model_path = os.path.join(\"files\", \"model.h5\")\n",
        "    csv_path = os.path.join(\"files\", \"log.csv\")\n",
        "\n",
        "    # load dataset\n",
        "    train_x, valid_x, test_x = load_data(dataset_path)\n",
        "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
        "\n",
        "    train_ds = tf_dataset(train_x, batch=hp[\"batch_size\"])\n",
        "    valid_ds = tf_dataset(valid_x, batch=hp[\"batch_size\"])\n",
        "\n",
        "    # define model\n",
        "    model = ViT(hp)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
        "        metrics=[\"acc\"]\n",
        "    )\n",
        "\n",
        "    # define callbacks\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
        "        CSVLogger(csv_path),\n",
        "        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
        "    ]\n",
        "\n",
        "    # train model\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        epochs=hp[\"num_epochs\"],\n",
        "        validation_data=valid_ds,\n",
        "        callbacks=callbacks\n",
        "    )"
      ],
      "metadata": {
        "id": "atYyCdHxZAze"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}