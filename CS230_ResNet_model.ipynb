{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ13_a3m-PoI"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import cv2\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# Import Keras modules and its important APIs\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CLUCXGk5u7A",
        "outputId": "9f20b237-2650-414e-eb07-8571b2e9c230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD NON AUGMENTED DATA"
      ],
      "metadata": {
        "id": "49EK_duWuhso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read img_name and label from csv file then upload image with img_name\n",
        "line_count = 0\n",
        "total_images = 0\n",
        "path = '/content/gdrive/MyDrive/CS230/CNN_images/all_cropped_images2'\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "with open('/content/gdrive/MyDrive/CS230/CNN_images/filenames_and_labels_no_duplicates.csv', newline='') as csvfile:\n",
        "  csvfile = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "  for row in csvfile:\n",
        "    # if line_count > 350:\n",
        "    #   break \n",
        "    if line_count == 0:\n",
        "      line_count += 1\n",
        "    else:\n",
        "      img_name = row[1]\n",
        "      if os.path.isfile(os.path.join(path, img_name)):\n",
        "        img = cv2.imread(os.path.join(path, img_name))\n",
        "        X.append(img)\n",
        "        y.append(row[2])\n",
        "        total_images += 1\n",
        "      else:\n",
        "        pass\n",
        "      line_count += 1\n",
        "    if line_count % 100 == 0:\n",
        "      print(line_count)\n",
        "  print(f'Processed {line_count} lines from CSV file.')\n",
        "  print(f'Imported {total_images} images into input X.')"
      ],
      "metadata": {
        "id": "jGx_x2twOFX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a save of X and y to avoid having to reload it entirely\n",
        "pickle.dump( X, open( \"/content/gdrive/MyDrive/CS230/CNN_images/input_images.p\", \"wb\" ) )\n",
        "pickle.dump( y, open( \"/content/gdrive/MyDrive/CS230/CNN_images/labels.p\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "7Yz62PFT_bnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pickle.load( open( \"/content/gdrive/MyDrive/CS230/input_images.p\", \"rb\" ) )\n",
        "y = pickle.load( open( \"/content/gdrive/MyDrive/CS230/labels.p\", \"rb\" ) )"
      ],
      "metadata": {
        "id": "2Mr2MoANNpD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT AUGMENTED DATA"
      ],
      "metadata": {
        "id": "aF0ST9vVue0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/aug_data_X.p\", \"rb\" ) )\n",
        "y = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/aug_data_y.p\", \"rb\" ) )"
      ],
      "metadata": {
        "id": "aPJnyPpqQfLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train - test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)\n",
        "print(f\"Dims of training examples: {np.shape(x_train)}\")\n",
        "print(f\"Dims of testing examples: {np.shape(x_test)}\")"
      ],
      "metadata": {
        "id": "8tSFAXpvtBDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(y_test)) # print all unique values in y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmhr912qRatu",
        "outputId": "db0b51a6-977f-44aa-993d-b0136c2f30fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to np arrays\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "# Converts y_train and y_test data from '0.0' to 0.0 then to 0\n",
        "y_train = np.array(y_train, dtype=\"float\")\n",
        "y_train = np.array(y_train, dtype=\"int\")\n",
        "y_test = np.array(y_test, dtype=\"float\")\n",
        "y_test = np.array(y_test, dtype=\"int\")\n",
        "\n",
        "print(set(y_train)) # print all unique values in y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f6ApBwa9Tlx",
        "outputId": "c665ae83-40e9-435e-8618-cb149e3b0191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUGMENTED DATA"
      ],
      "metadata": {
        "id": "YO8VYVAASJiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a save of inputs and labels for train and test sets to avoid having to pre-process them again\n",
        "pickle.dump( x_train, open( \"/content/gdrive/MyDrive/CS230_VIT/aug_x_train.p\", \"wb\" ) )\n",
        "pickle.dump( x_test, open( \"/content/gdrive/MyDrive/CS230_VIT/aug_x_test.p\", \"wb\" ) )\n",
        "pickle.dump( y_train, open( \"/content/gdrive/MyDrive/CS230_VIT/aug_y_train.p\", \"wb\" ) )\n",
        "pickle.dump( y_test, open( \"/content/gdrive/MyDrive/CS230_VIT/aug_y_test.p\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "OO8qt21YRoq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a save of inputs and labels for train and test sets if necessary\n",
        "x_train = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/aug_x_train.p\", \"rb\" ) )\n",
        "x_test = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/aug_x_test.p\", \"rb\" ) )\n",
        "y_train = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/aug_y_train.p\", \"rb\" ) )\n",
        "y_test = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/aug_y_test.p\", \"rb\" ) )"
      ],
      "metadata": {
        "id": "lrdHWhQqR5Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NON AUGMENTED DATA"
      ],
      "metadata": {
        "id": "wxN1j97ySLRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a save of inputs and labels for train and test sets to avoid having to pre-process them again\n",
        "pickle.dump( x_train, open( \"/content/gdrive/MyDrive/CS230/CNN_images/x_train.p\", \"wb\" ) )\n",
        "pickle.dump( x_test, open( \"/content/gdrive/MyDrive/CS230/CNN_images/x_test.p\", \"wb\" ) )\n",
        "pickle.dump( y_train, open( \"/content/gdrive/MyDrive/CS230/CNN_images/y_train.p\", \"wb\" ) )\n",
        "pickle.dump( y_test, open( \"/content/gdrive/MyDrive/CS230/CNN_images/y_test.p\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "lG_p3MJxAYvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a save of inputs and labels for train and test sets if necessary\n",
        "x_train = pickle.load( open( \"/content/gdrive/MyDrive/CS230/x_train.p\", \"rb\" ) )\n",
        "x_test = pickle.load( open( \"/content/gdrive/MyDrive/CS230/x_test.p\", \"rb\" ) )\n",
        "y_train = pickle.load( open( \"/content/gdrive/MyDrive/CS230/y_train.p\", \"rb\" ) )\n",
        "y_test = pickle.load( open( \"/content/gdrive/MyDrive/CS230/y_test.p\", \"rb\" ) )"
      ],
      "metadata": {
        "id": "XOm7slkYArtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYPER-PARAMETERS"
      ],
      "metadata": {
        "id": "vq-1QzxgSOR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # original ResNet paper uses batch_size = 128\n",
        "epochs = 85             # EDIT\n",
        "num_classes = 20 \n",
        "  \n",
        "# Data Preprocessing \n",
        "subtract_pixel_mean = True\n",
        "n = 3                   # EDIT\n",
        "  \n",
        "# Select ResNet Version\n",
        "version = 2\n",
        "  \n",
        "# Computed depth of ResNet model\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "  \n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet % dv % d' % (depth, version)\n",
        "  \n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "  \n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "  \n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis = 0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "  \n",
        "# Print Training and Test Samples \n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "  \n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "8GA3YHckxoQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting learning rate for different number of Epochs\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "metadata": {
        "id": "C3cgxZapyDYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic ResNet Building Block\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16, # EDIT\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=False):\n",
        "    conv=Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "  \n",
        "    x=inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "B0QDo-NG0f40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet V2 architecture\n",
        "def resnet_v2(input_shape, depth, num_classes=20): \n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n + 2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16 #8 # 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "  \n",
        "    inputs = Input(shape=input_shape)\n",
        "    # V2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "  \n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "  \n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "  \n",
        "        num_filters_in = num_filters_out\n",
        "  \n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "    # outputs = idx_max(outputs)\n",
        "    print(\"outputs are \", outputs)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "11wIgauj1KMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function \n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape = input_shape, depth = depth)\n",
        "\n",
        "# else:\n",
        "#     model = resnet_v1(input_shape = input_shape, depth = depth)\n",
        "\n",
        "model.compile(loss ='categorical_crossentropy',\n",
        "              optimizer = Adam(learning_rate = lr_schedule(0)),\n",
        "              metrics =['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision(),\n",
        "                        tf.keras.metrics.FalseNegatives(), \n",
        "                        tf.keras.metrics.TrueNegatives(), \n",
        "                        tf.keras.metrics.FalsePositives(),\n",
        "                        tf.keras.metrics.TruePositives()])\n",
        "\n",
        "model.summary()\n",
        "print(model_type)\n",
        "  \n",
        "# Prepare model saving directory. \n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'eye_classifier_% s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "  \n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath = filepath,\n",
        "                             monitor ='val_acc',\n",
        "                             verbose = 1,\n",
        "                             save_best_only = True)\n",
        "  \n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "  \n",
        "lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1),\n",
        "                               cooldown = 0,\n",
        "                               patience = 5,\n",
        "                               min_lr = 0.5e-6)\n",
        "  \n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "  \n",
        "# Run training\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_data =(x_test, y_test),\n",
        "          shuffle = True,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose = 1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "metadata": {
        "id": "IHgQ4Fvz2uwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/gdrive/MyDrive/CS230_VIT/resnet_model')"
      ],
      "metadata": {
        "id": "x-gYd-BYuTGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = keras.models.load_model('/content/gdrive/MyDrive/CS230_VIT/resnet_model')"
      ],
      "metadata": {
        "id": "ZloVXGOzvr_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump( scores, open( \"/content/gdrive/MyDrive/CS230_VIT/scores.p\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "DJDeOYL1Z1Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ResNet trained model\n",
        "resnet_model = keras.models.load_model('/content/gdrive/MyDrive/CS230_VIT/resnet_model')\n",
        "resnet_scores = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/scores.p\", \"rb\" ) )"
      ],
      "metadata": {
        "id": "I8yRzRqYYiwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess x_test the way it is preprocessed for the ResNet\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "  \n",
        "# Subtract pixel mean\n",
        "x_train_mean = np.mean(x_train, axis = 0)\n",
        "x_train -= x_train_mean\n",
        "x_test -= x_train_mean"
      ],
      "metadata": {
        "id": "zwPbnH9bYse5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain y_pred\n",
        "y_pred = resnet_model.predict(x_test)"
      ],
      "metadata": {
        "id": "AvZ82G6BYt4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change y_pred from one-hot to int\n",
        "y_pred_toInt = []\n",
        "for i in range(np.shape(y_pred)[0]):\n",
        "  y_pred_toInt.append(np.argmax(y_pred[i]))"
      ],
      "metadata": {
        "id": "lrBDQ-NyY1f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick check for accuracy\n",
        "wrong = 0\n",
        "correct = 0\n",
        "for i in range(np.shape(y_pred_toInt)[0]):\n",
        "  if y_pred_toInt[i] != y_test[i]:\n",
        "    wrong += 1\n",
        "  else:\n",
        "    correct += 1\n",
        "print(\"wrong: \", wrong)\n",
        "print(\"Correct: \", correct)"
      ],
      "metadata": {
        "id": "KPhTZz_mY3DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save y_test and y_pred as int\n",
        "pickle.dump( y_test, open( \"/content/gdrive/MyDrive/CS230_VIT/y_test_int_resnet.p\", \"wb\" ) )\n",
        "pickle.dump( y_pred_toInt, open( \"/content/gdrive/MyDrive/CS230_VIT/y_pred_int_resnet.p\", \"wb\" ) )"
      ],
      "metadata": {
        "id": "bXAXgjSSY4de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load y_test and y_pred as int\n",
        "y_test = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/y_test_int_resnet.p\", \"rb\" ) )\n",
        "y_pred = pickle.load( open( \"/content/gdrive/MyDrive/CS230_VIT/y_pred_int_resnet.p\", \"rb\" ) )"
      ],
      "metadata": {
        "id": "8t3hlpKLY6IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "y_true = y_test\n",
        "y_pred = y_pred # use y_pred_toInt if bypassing pickle files\n",
        "\n",
        "categories = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
        "\n",
        "cf_matrix = confusion_matrix(y_true, y_pred, labels = categories)\n",
        "# print(cf_matrix) # not necessary\n",
        "\n",
        "row_sums = cf_matrix.sum(axis=1)\n",
        "df_norm_col = cf_matrix / row_sums[:, np.newaxis]\n",
        "\n",
        "ax = sns.heatmap(df_norm_col, cmap='viridis', annot=False)\n",
        "ax.xaxis.tick_top()"
      ],
      "metadata": {
        "id": "kebbmrovQ4Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change y_test and y_pred to one-hot\n",
        "y_onehot_test = keras.utils.to_categorical(y_test, 20)\n",
        "y_onehot_pred = keras.utils.to_categorical(y_pred, 20)"
      ],
      "metadata": {
        "id": "tGmZ98ACZd8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "# Calculate the AUC of the ROC\n",
        "n_classes = 20\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    yot = y_onehot_test[:][i]\n",
        "    yop = y_onehot_pred[:][i]\n",
        "    fpr[i], tpr[i], _ = roc_curve(yot, yop)\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "roc_auc"
      ],
      "metadata": {
        "id": "Kpi7WsgnZknO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick check for accuracy\n",
        "sum(roc_auc.values()) / n_classes"
      ],
      "metadata": {
        "id": "Bf_Te3WAZlp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
        "for class_id, color in zip(range(n_classes), colors):\n",
        "    RocCurveDisplay.from_predictions(\n",
        "        y_onehot_test[:, class_id],\n",
        "        y_onehot_pred[:, class_id],\n",
        "        name=f\"ROC curve for {class_id}\",\n",
        "        color=color,\n",
        "        ax=ax,\n",
        "    )\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")"
      ],
      "metadata": {
        "id": "qkEad18YZm92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydot\n",
        "!pip install pydotplus\n",
        "!pip install graphviz"
      ],
      "metadata": {
        "id": "Iuv-H7TZtsbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_img_file = 'model.png'\n",
        "tf.keras.utils.plot_model(resnet_model, to_file=model_img_file, \n",
        "                          show_shapes=True, \n",
        "                          show_layer_activations=True, \n",
        "                          show_dtype=True,\n",
        "                          show_layer_names=True )"
      ],
      "metadata": {
        "id": "4vzyADY-twhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visualkeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4yBp9HHuLs1",
        "outputId": "abd6395b-56ab-4241-bc91-50fb4e133263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting visualkeras\n",
            "  Downloading visualkeras-0.0.2-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from visualkeras) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from visualkeras) (1.22.4)\n",
            "Collecting aggdraw>=1.3.11 (from visualkeras)\n",
            "  Downloading aggdraw-1.3.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.0/993.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aggdraw, visualkeras\n",
            "Successfully installed aggdraw-1.3.16 visualkeras-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import visualkeras"
      ],
      "metadata": {
        "id": "FNlxVbgYuhXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(resnet_model,legend=True, draw_volume=True)"
      ],
      "metadata": {
        "id": "xjZYPUlfuQgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}